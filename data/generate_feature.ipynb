{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T08:06:46.600795023Z",
     "start_time": "2024-09-14T08:06:38.995692289Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor,CLIPModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import timm\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "from transformers import AutoImageProcessor, ResNetModel, AutoModelForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e03180-d1d9-4b4e-85a9-141fe13ea9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "global df\n",
    "df = pd.read_csv(\"/hdd/wmh/Cookie/src_data/final/data.csv\")[['sample', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967f3572-4ca8-4615-9ed7-b18ece144278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T10:34:40.008508763Z",
     "start_time": "2024-07-12T10:34:40.004732068Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a3c573-1403-42c4-ab89-83dc45aa0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mel_images(src_path: str, save_path: str):\n",
    "    # 加载音频文件\n",
    "    for _, row in df.iterrows():\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        # 加载音频\n",
    "        audio_file = f\"{src_path}/{sample}.mp3\"\n",
    "        y, sr = librosa.load(audio_file)\n",
    "    \n",
    "        # 计算 mel 频谱图\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        \n",
    "        # 将 mel 频谱图转换为 dB 尺度\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "        # 使用 librosa 显示 mel 频谱图\n",
    "        plt.figure()\n",
    "        librosa.display.specshow(log_mel_spectrogram, sr=sr)\n",
    "        plt.savefig('mel_spectrogram.png', bbox_inches='tight')\n",
    "        matplotlib.pyplot.close()\n",
    "        # 使用 cv2 保存 224x224 大小的图片\n",
    "        img = cv2.imread('mel_spectrogram.png')\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        cv2.imwrite(f\"{save_path}/{sample}.png\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcd9c5-fbad-43cb-91bd-5913d8ab0572",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3199f939-e016-4508-a89c-b66c120abf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CLIP_features(src_path: str, save_path: str):\n",
    "    model = CLIPModel.from_pretrained(\"/hdd/wmh/clip/\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"/hdd/wmh/clip/\")\n",
    "    clip_features = []\n",
    "    global df\n",
    "    for _, row in df.iterrows():\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        img = Image.open(f\"{src_path}{sample}.png\")\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs).squeeze().cpu() # [1, 512] --> [512] \n",
    "        clip_features.append([sample, label] + list(image_features.detach().numpy()))\n",
    "    df = pd.DataFrame(clip_features)\n",
    "    df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07da2f1-bdee-40d2-b03f-fae5095f6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mvitv_features(src_path: str, save_path: str):\n",
    "    model = model = timm.create_model(\n",
    "        'mvitv2_base_cls.fb_inw21k',\n",
    "        pretrained=True,\n",
    "        num_classes=0,  # remove classifier nn.Linear\n",
    "    ).to(device)\n",
    "    model = model.eval()\n",
    "    # get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "    \n",
    "    clip_features = []\n",
    "    global df\n",
    "    for _, row in df.iterrows():\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        img = Image.open(f\"{src_path}{sample}.png\")\n",
    "        output = model(transforms(img).unsqueeze(0).to(device))  # output is (batch_size, num_features) shaped tensor\n",
    "        # or equivalently (without needing to set num_classes=0)\n",
    "        output = model.forward_features(transforms(img).unsqueeze(0).to(device))\n",
    "        # output is unpooled, a (1, 50, 768) shaped tensor\n",
    "        output = model.forward_head(output, pre_logits=True).cpu().squeeze()\n",
    "        clip_features.append([sample, label] + list(output.detach().numpy()))\n",
    "    df = pd.DataFrame(clip_features)\n",
    "    df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54b9762-fb64-48d8-92b3-93691922ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ResNet_features(src_path: str, save_path: str):\n",
    "    processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "    model = ResNetModel.from_pretrained(\"microsoft/resnet-50\").to(device)\n",
    "    \n",
    "    Resnet_features = []\n",
    "    global df\n",
    "    for _, row in df.iterrows():\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        img = Image.open(f\"{src_path}{sample}.png\")\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model(**inputs).pooler_output.squeeze().cpu() # [1, 512] --> [512] \n",
    "            # print(model(**inputs).pooler_output.squeeze().cpu().shape)\n",
    "            # assert 0\n",
    "        Resnet_features.append([sample, label] + list(image_features.detach().numpy()))\n",
    "    df = pd.DataFrame(Resnet_features)\n",
    "    df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea966c8-257e-4057-804a-b7e6fcfcc531",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55506445-ae2f-44fa-a4dd-ad626fc9cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings.cpu() * input_mask_expanded.cpu(), 1) / torch.clamp(input_mask_expanded.cpu().sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16738f563fd960",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_features(model_path: str, src_path: str,  save_path: str):\n",
    "    # 加载模型\n",
    "    model = AutoModel.from_pretrained(model_path).to(device)\n",
    "    Tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    text_features = []\n",
    "    print(device)\n",
    "    global df\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        with open(f\"{src_path}/{sample}.txt\") as f:\n",
    "            text = f.read().replace(\"\\t\", ' ')\n",
    "        with torch.no_grad():\n",
    "            if \"mpnet\" in model_path:\n",
    "                inputs = Tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_output = model(**inputs)\n",
    "                embeddings = mean_pooling(model_output, inputs['attention_mask'])\n",
    "            else:\n",
    "                try:\n",
    "                    inputs = Tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                    embeddings = model(**inputs)['pooler_output'] # [1, 512] --> [512] \n",
    "                except:\n",
    "                    print(sample)\n",
    "                    print(label)\n",
    "                    print(text)\n",
    "                    assert 0\n",
    "                \n",
    "        text_features.append([sample, label] + list(embeddings.squeeze().cpu().detach().numpy()))\n",
    "    res_df = pd.DataFrame(text_features)\n",
    "    res_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3997f74-a537-4ef9-be8c-c77217ee992f",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206272cb-9fdf-402c-8394-5f446aa11455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, AutoModel, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91274e25-be10-4e44-a481-6e07c55ad49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genereate_audio_features(model_path: str, src_path: str,  save_path: str):\n",
    "    global df\n",
    "    model = AutoModel.from_pretrained(model_path).to(device)\n",
    "    processor = AutoFeatureExtractor.from_pretrained(model_path)\n",
    "    audio_features = []\n",
    "    \n",
    "    max_len = 16000 * 30\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        wav, sr = librosa.load(f\"{src_path}/{sample}.mp3\", sr=16000)\n",
    "        # k clips\n",
    "        wav_length = wav.shape[0]\n",
    "        clips = [(i * max_len, min((i + 1) * max_len, wav_length)) for i in range(int(wav_length / max_len) + 1)]\n",
    "        clip_features = []\n",
    "        \n",
    "        for clip in clips:\n",
    "            inputs = processor(wav[clip[0]: clip[1]], sampling_rate=16000, return_tensors=\"pt\").input_features.cuda()\n",
    "            decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.cuda(), decoder_input_ids=decoder_input_ids.cuda())\n",
    "            out_feature = outputs['last_hidden_state'].mean(dim=1)\n",
    "            # print(out_feature.shape)\n",
    "            clip_features.append(out_feature.squeeze().cpu())\n",
    "        # 将所有片段的特征取平均\n",
    "        avg_feature = np.mean(clip_features, axis=0)\n",
    "        audio_features.append([sample, label] + list(avg_feature))\n",
    "    res_df = pd.DataFrame(audio_features)\n",
    "    res_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5589a998-046a-458d-89b9-120299751078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genereate_hubert_features(model_path: str, src_path: str,  save_path: str):\n",
    "    global df\n",
    "    model = AutoModel.from_pretrained(model_path).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    audio_features = []\n",
    "    \n",
    "    max_len = 16000 * 30\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        wav, sr = librosa.load(f\"{src_path}/{sample}.mp3\", sr=16000)\n",
    "        # k clips\n",
    "        wav_length = wav.shape[0]\n",
    "        clips = [(i * max_len, min((i + 1) * max_len, wav_length)) for i in range(int(wav_length / max_len) + 1)]\n",
    "        clip_features = []\n",
    "        \n",
    "        for clip in clips:\n",
    "            inputs = processor(wav[clip[0]: clip[1]], sampling_rate=16000, return_tensors=\"pt\").input_values.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.cuda())\n",
    "            out_feature = outputs['last_hidden_state'].mean(dim=1)\n",
    "            # print(out_feature.shape)\n",
    "            clip_features.append(out_feature.squeeze().cpu())\n",
    "        # 将所有片段的特征取平均\n",
    "        avg_feature = np.mean(clip_features, axis=0)\n",
    "        audio_features.append([sample, label] + list(avg_feature))\n",
    "    res_df = pd.DataFrame(audio_features)\n",
    "    res_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39aa34c1-aa34-43d6-83ad-c05c7a8e6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genereate_wav2vec_features(src_path: str,  save_path: str):\n",
    "    global df\n",
    "    model = AutoModel.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    audio_features = []\n",
    "    \n",
    "    max_len = 16000 * 30\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        sample = row['sample']\n",
    "        label = row['label']\n",
    "        wav, sr = librosa.load(f\"{src_path}/{sample}.mp3\", sr=16000)\n",
    "        # k clips\n",
    "        wav_length = wav.shape[0]\n",
    "        clips = [(i * max_len, min((i + 1) * max_len, wav_length)) for i in range(int(wav_length / max_len) + 1)]\n",
    "        clip_features = []\n",
    "        \n",
    "        for clip in clips:\n",
    "            inputs = feature_extractor(wav[clip[0]: clip[1]], sampling_rate=16000, return_tensors=\"pt\").input_values.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.cuda())\n",
    "            out_feature = outputs['last_hidden_state'].mean(dim=1)\n",
    "            # print(out_feature.squeeze().shape)\n",
    "            clip_features.append(out_feature.squeeze().cpu().numpy())\n",
    "        # 将所有片段的特征取平均\n",
    "        # print(np.array(clip_features).shape)\n",
    "        avg_feature = np.mean(np.array(clip_features), axis=0)\n",
    "        audio_features.append([sample, label] + list(avg_feature))\n",
    "    res_df = pd.DataFrame(audio_features)\n",
    "    res_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86e29ec8-a060-4627-bc1d-87774b3f6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "srcpath = \"/hdd/wmh/Cookie/src_data/audio/\"\n",
    "\n",
    "savepath = \"/hdd/wmh/Cookie/feature/audio-wav2vec-base.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af4dd7d7-7e68-4f4e-9e73-3b35fcb4709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/wmh/.conda/envs/nya/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "549it [03:51,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "genereate_wav2vec_features(srcpath, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee14aa-126b-4d0d-b556-7076519f26b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1ccb7-e258-4d7e-bbb8-4b3862e55ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871b582-af16-44fc-a6d0-848ab844a1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bdd07df-f97f-4f2f-999b-e40c0f7b8e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "audio_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "754346ec-bee1-4e65-b09d-f3cd0a8431e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 16000 * 30\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    sample = row['sample']\n",
    "    label = row['label']\n",
    "    wav, sr = librosa.load(f\"{src_path}/{sample}.mp3\", sr=16000)\n",
    "    # k clips\n",
    "    wav_length = wav.shape[0]\n",
    "    clips = [(i * max_len, min((i + 1) * max_len, wav_length)) for i in range(int(wav_length / max_len) + 1)]\n",
    "    clip_features = []\n",
    "    \n",
    "    for clip in clips:\n",
    "        inputs = feature_extractor(wav[clip[0]: clip[1]], sampling_rate=16000, return_tensors=\"pt\").input_values.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.cuda())\n",
    "        out_feature = outputs['last_hidden_state'].mean(dim=1)\n",
    "        print(out_feature.squeeze().shape)\n",
    "        clip_features.append(out_feature.squeeze().cpu())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "494d1fa4-70db-466d-856d-1443affa0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "for clip in clip_features:\n",
    "    print(clip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6836b893-1818-45dd-a612-2de32cf12e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n"
     ]
    }
   ],
   "source": [
    "# 将所有片段的特征取平均\n",
    "print(np.array(clip_features).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d32ecf2d-fae7-45fa-865e-33fce82bd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature = np.mean(np.array(clip_features), axis=0)\n",
    "audio_features.append([sample, label] + list(avg_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d300fe72-9871-4e91-b473-c8c151bb386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_feature.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
